---
title: "Capital Bikeshare System Analysis"
author: "Ross Bradley"
date: "October 15, 2018"
output: 
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 
```{r read_in, echo=FALSE, message=FALSE, results=FALSE}

library(ggplot2)
library(dplyr)
library(RColorBrewer)
library(GGally)
library(scatterplot3d)

bike.share <- read.csv("C:/Users/rbbra/Documents/R/Capstone/DC-Bike-Share-Data/Bike-Sharing-Dataset/day.csv")
hourly <- read.csv("C:/Users/rbbra/Documents/R/Capstone/DC-Bike-Share-Data/Bike-Sharing-Dataset/hour.csv") ## Import dataset
names(bike.share) ##names of the columns

dteday <- as.Date(bike.share$dteday)
weathersit <- factor(bike.share$weathersit)
year <- factor(bike.share$yr)
month <- factor(bike.share$mnth)
season <- factor(bike.share$season)
holiday <- factor(bike.share$holiday)
weekday <- factor(bike.share$weekday)
workday <- factor(bike.share$workday)
cnt <- bike.share$cnt
```



# Washington D.C. Bike Share Analysis


"Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. 

Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data." - [UC - Irvine](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)

Below is a proposition to analyze a dataset provided by the Capital BikeShare system will be the foundation of my Capstone Project for Springboard's Intro to Data Science course. This dataset provides information on the number of users, date information, and the weather details broken down by hour and day. The data was collected between 2011 and 2012 in the Washington D.C. area.


## Proposition

A company needs to know its bottom line. Using the data above, I plan to predict with confidence the company's average number of users moving forward. Capital Bikeshare system will find this information important to know to predict their revenue. With a confident prediction, the company will have an estimate of how many riders will use their service and have a baseline of how much revenue they can expect to generate. Knowing your expected returns allows a company to know how much and when they can take risks. 

The idea is to take the dataset and identify what effect each weather condition has on rider turnout. Does heavy rain keep people away more than 90 degree weather? How many registered users are guaranteed to ride each day? I'm proposing to use the data available to find the variables that have the strongest effect on user turnout and build a model to accurately give a count of users based on expected future weather patterns.

## Data & Variables

There are 2 datasets (daily and hourly) we have to analyze and 16 variables in the original sets. We are provided 2 calendar years of D.C. weather data and the daily or hourly count of users on the company's bikes on each interval given the dataset. Because we care more about the overarching picture, our focus will be on the daily dataset.

7 of our variables are in regards to **time and date**. This includes the date, the day of the week, if it were a holiday, if it were a workday, month, season, and year (0 for 2011 and 1 for 2012).  5 variables define the **weather**: temperature, "real feel" temperature, weather situation put into 4 categories based on level of severity, humididy, and windspeed. Lastly we have 3 variables categorizing each interval's **user count**, broken out by number of registered riders, casual riders, and the sum of the two. 

Our output variable and the focus of the project will be the **count**. This is the sum of both the registered and casual columns which gives the total number of riders in the time interval. Our function will primarily be built off weather and date variables. Our initial assumption being that weather and date both play a role in predicting the daily number of riders.


## Data wrangling

We are lucky to be working with a dataset that only has 1 missing value. We had a 0 value humidity on 1 date on the daily dataset. Luckily, this value could be calculated and entered based on the data from the hourly dataset. No assumptions needed to be made about the missing value since the formula for how the humidity on the daily dataset was the average of humidity values for the day in the hourly dataset. 

The season, year, month, hour, weekday, workingday, and weather situation are all collected as integer values between 0 and their respective maximum levels. We must provide code to R so that these values are interpreted as categorical instead of numerical numbers. The season column has had its original values converted from 1-4 to "Spring", "Summer", etc. for added clarity. 

2 additional rows were created, tempC and tempF. The initial temperature variable is a proportion calculated by the data collector. The proportion has been reconverted to the real temperature in Celcius and Fahrenheit for clarity. 

``` {r results = FALSE}
#Updating Season Columns
as.character(bike.share$season,stringsAsFactors=FALSE)
```
```{r}
winter.vec <- bike.share$season =="1"
bike.share$season[winter.vec] <- "Winter"

spring.vec <- bike.share$season =="2"
bike.share$season[spring.vec] <- "Spring"

summer.vec <- bike.share$season =="3"
bike.share$season[summer.vec] <- "Summer"

fall.vec<- bike.share$season =="4"
bike.share$season[fall.vec] <- "Fall"


#Updating temp column to degrees in Fahrenheit
temp.form <- function(x){
  output <- ((x*47)-8)*(9/5)+32
  return(output)
}

bike.share$temp <- temp.form(x = bike.share$temp)

#Updating hum column to percentages
temp.hum <- function(x){
  output <- (x*100)
  return(output)
}

bike.share$hum <- temp.hum(x = bike.share$hum)
```

#Data Story

##Visualization
We start by looking at some of the direct correlations between the variables and the count of riders to see which has the most direct effect on rider count:

```{r warning = FALSE}
tempcorr <- cor.test(x = bike.share$tempF, y = bike.share$cnt, method = "spearman")
windcorr <- cor.test(x = bike.share$windspd, y = bike.share$cnt, method = "spearman")
humcorr <- cor.test(x = bike.share$hum, y = bike.share$cnt, method = "spearman")
sitcorr <- cor.test(x = bike.share$weathersit, y = bike.share$cnt, method = "spearman")
```

```{r}
windcorr
humcorr
tempcorr
sitcorr
```

One can see that temperature has the strongest and only seemingly positive correlation. The rest are negative. A closer look at the visualizations of the data may prove helpful in understanding their relationship. 

```{r message=FALSE}
library(ggplot2)
library(ggpmisc)
#Total Riders by humidity.
#shows slight negative trend. High temperature is above the line, cool is below.

ggplot(bike.share, aes(x=hum, y=cnt, col = temp))+
  geom_point()+
  labs(x="Humidity (%)", y = "Total Riders")+
  scale_color_gradientn(colors = brewer.pal(9, "YlOrRd"))+
  #facet_grid(weathersit~.)+
  geom_smooth(method = "lm", formula = y ~ x)
```

The chart above shows that humidity trends slightly negatively with total riders. Colored by temperature, here we can see that high temperatures are above our trend line and lower temperatures are mostly below. This could mean that temperature correlates positively with rider count. 

Let's continue to review temperature as a variable to predict total riders in a day. The below chart will plot total riders in a given day. We think that temperature will play a role, but  but also break down the points by season. 
```{r message=FALSE}
#Total Riders by temperature, following a log function of the data. 
#Slight trend noticed that very high T is below the trend line
# y = -167 + 78.5x
library(ggpmisc)
ggplot(bike.share, aes(x=bike.share$temp, y=cnt, col = hum))+
  geom_point()+
  labs(x="Humidity (%)", y = "Total Riders")+
  geom_smooth(method = "lm", formula = y~x)+
  stat_poly_eq(parse =T, aes(label= ..eq.label..), formula = y~x)
```

We can see that temperature tends to trend upwards, but we know a 1:1 correlation can't exist. Eventually it will get too hot for bike sharing! Because we know this isn't a perfect relationship, let's look at the data given a cubic function to estimate the outcome. We also want to include the weather situation in these visualizations to see how harsher weather can effect the rider count. We would suspect that level 1 would be higher up where level 3 would yield less riders.

```{r message = FALSE}
#Same as above but with a cubic function showing that as temperature increases too much we have a negative trend!
# y = 1460 -153x + 6.51x^2 - .0492x^3
formula <- y ~ poly(x,3, raw = TRUE)

ggplot(bike.share, aes(x=temp, y=cnt, col = weathersit))+
  geom_point(aes(color = factor(weathersit)))+
  labs(x="Temperature (F)", y = "Total Riders")+
  geom_smooth(method = "glm", formula = formula)+
  stat_poly_eq(parse =T, aes(label= ..eq.label..), formula = formula)
```

From the above chart we see that after the temperature reaches ~75 degrees our total number of bikers tends to decrease. We can also see that weather categories 1 and 2 are both above and below the line of best fit. Category 3 is under the curve in all instances which is what we expected. 

What is very interesting is that even with a cubic function, the data is very far parsed. There must be more variables that come into play when people choose to go ride. Let's investigate further.

Next we will visualize the average wind speed of the day against the total rider count. 

```{r}
#windspeed verse count
ggplot(bike.share, aes(x=bike.share$windspd, y = cnt))+
  geom_point()+
  geom_smooth(method = "lm", formula = y~x)+
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Linear Model Fitted to Data")
```

Here we see a slight negative trend here as well. Temperature seems to be our best indicator, both indicated by its correlation and the graph, but if we combine these variables together we may have a clear picture.

#Analysis

Let's begin by splitting our data into test and training datasets. This will allow us to build a model based on a sample of data and then test it on the full set. Our test set will be made up of approximately 25% of our data while the traingin set will be made out of the remaining 75%.

We will then test a function that involves using all variables to test 

```{r}
#split data by days of the month to create test and train sets
train <- bike.share[as.integer(substr(bike.share$dteday, 9,10)) < 24, ]
test <- bike.share[as.integer(substr(bike.share$dteday, 9,10)) > 23,]

nrow(train)/ nrow(bike.share) #75%
nrow(test)/ nrow(bike.share) #25%


#first model will include all variables
fit_1 <- lm(cnt ~ season + yr + mnth + holiday + weekday + 
             workday + weathersit + temp + hum + windspd, data= train)
summary(fit_1)  #month, holiday, workday: insignificant



tst.cnt <- predict(fit_1, test)
test$tst.cnt <- tst.cnt
tst.MSE <- sum((test$cnt - test$tst.cnt)^2)/nrow(test) ##MSE Formula
tst.MSE
tst.result = ggplot(test,aes(cnt,tst.cnt))+geom_point()
tst.result
test$tst.cnt[test$tst.cnt < 0] = 0
tst.MSE <- sum((test$cnt - test$tst.cnt)^2)/nrow(test)
tst.MSE  
tst.result <- ggplot(test,aes(cnt,tst.cnt))+geom_point()
tst.result 

#reduce model by what was found insignificant in the last model
fit_2 <- lm(cnt ~ season + yr + weathersit + workday + temp + hum + windspd, train)
summary(fit_2)  #workday: insignificant


tst.cnt <- predict(fit_2, test)
test$tst.cnt <- tst.cnt
tst.MSE <- sum((test$cnt - test$tst.cnt)^2)/nrow(test) ##MSE Formula
tst.MSE
tst.result = ggplot(test,aes(cnt,tst.cnt))+geom_point()
tst.result
test$tst.cnt[test$tst.cnt < 0] = 0
tst.MSE <- sum((test$cnt - test$tst.cnt)^2)/nrow(test)
tst.MSE  
tst.result <- ggplot(test,aes(cnt,tst.cnt))+geom_point()
tst.result 


#reduce model by what was found insignificant in the last model
fit_3 <- lm(cnt ~ season + yr + weathersit + temp + hum + windspd, train)
summary(fit_3)  #all significant


tst.cnt <- predict(fit_3, test)
test$tst.cnt <- tst.cnt
tst.MSE <- sum((test$cnt - test$tst.cnt)^2)/nrow(test) ##MSE Formula
tst.MSE
tst.result = ggplot(test,aes(cnt,tst.cnt))+geom_point()
tst.result
test$tst.cnt[test$tst.cnt < 0] = 0
tst.MSE <- sum((test$cnt - test$tst.cnt)^2)/nrow(test)
tst.MSE  
tst.result <- ggplot(test,aes(cnt,tst.cnt))+geom_point()+
  stat_poly_eq(parse =T, aes(label= ..eq.label..), formula = y~x)
tst.result 
```

Here we see that the MSE for the second model has the lowest value, meaning its plot is the most in tune with the data. The variables **season + yr + weathersit + workday + temp + hum + windspd** show the most significance in plotting our model. 

#In Retrospect
A further look into the data could have shown a more clear predictor of the data. A closer look at the number of riders in relation to the date shows a potential time series. Further analysis into time series packages could reveal more precise values, and the data points to being non-stationary by its rise over time. 

```{r echo=FALSE}
ggplot(bike.share, aes(x=dteday, y=cnt, col = tempF))+
  geom_point()+
  labs(x="Date", y = "Total Riders")+
  scale_color_gradientn(colors = brewer.pal(9, "YlOrRd"))+
  theme_minimal()+
  stat_smooth(method = "lm", formula =y~x)+
  theme(axis.title.x = element_text(angle = 90, hjust = 1))+
  stat_poly_eq(parse =T, aes(label= ..eq.label..), formula = y~x)
```

Some additional variables would be good to know for further analysis. For instance, we our number of riders is not defined as unique riders or repeat customers for the day. What proportion of DC's population is a registered user? How does that proportion change over this 2 year period? How quickly did the market saturate if it did? 

A deeper categorization of the weather category could also lead to more conclusive results. For weather to be broken into 4 categories, and a single category only referenced 3-4 times on the hourly dataset means better data collection could have been done in this particular field. 

##Conclusion

In conclusion we found that season, year, weather situation, workday, temperature, humidity, and wind speed were all significant factors in predicting the outcome of total riders. 